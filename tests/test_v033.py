"""v0.3.3 testleri â€” Teorik ve Biyolojik Ä°yileÅŸtirmeler."""

import sys, os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
torch.autograd.set_detect_anomaly(True)
import torch.nn.functional as F
from liquidnn import MiniLiquidGPT
from liquidnn.ode_cell import LiquidODECell
from liquidnn.plasticity import PlasticSynapse


# â•â•â• 1. Decoupled Hebbian Capacity â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_hebb_capacity_independent_of_w():
    """Hebb_capacity'nin W normundan baÄŸÄ±msÄ±z Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± doÄŸrula."""
    syn = PlasticSynapse(10, 10)
    syn.W.data.zero_()  # W normu 0 olursa eski mantÄ±kta plastisite de 0 olurdu
    assert syn.W.data.norm() == 0.0

    # Kapasite varsayÄ±lanÄ± ~0.69 (softplus(1.0))
    expected_cap = F.softplus(syn.hebb_capacity).item()
    
    pre = torch.randn(1, 10)
    post = torch.randn(1, 10) * 100  # Ã‡ok bÃ¼yÃ¼k gÃ¼ncelleme
    
    syn.update_hebb(pre, post)
    
    # Norm sÄ±fÄ±r olmamalÄ±, kapasitede kÄ±rpÄ±lmÄ±ÅŸ olmalÄ±
    actual_norm = syn.Hebb.norm().item()
    assert actual_norm > 0.0, "Hebb gÃ¼ncellenmeli (W=0 olsa bile)"
    assert actual_norm <= expected_cap * 1.01, "Kapasiteyi aÅŸmamalÄ±"
    print("âœ… hebb_capacity_independent_of_w")


# â•â•â• 2. MoE Unbalanced Updates â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_plasticity_moe_weight_scaling():
    """update_hebb fonksiyonunun moe_weight ile Ã¶lÃ§eklendiÄŸini doÄŸrula."""
    syn1 = PlasticSynapse(10, 10)
    syn2 = PlasticSynapse(10, 10)
    # Parametreleri eÅŸitle
    syn2.load_state_dict(syn1.state_dict())

    pre = torch.randn(1, 10)
    post = torch.randn(1, 10)

    # weight=1.0 (tam gÃ¼ncelleme)
    syn1.update_hebb(pre, post, moe_weight=1.0)
    norm1 = syn1.hebb_norm

    # weight=0.1 (Ã§ok kÃ¼Ã§Ã¼k gÃ¼ncelleme)
    syn2.update_hebb(pre, post, moe_weight=0.1)
    norm2 = syn2.hebb_norm

    assert norm1 > norm2, "moe_weight=1.0 daha Ã§ok gÃ¼ncellemeli"
    # Oran decay dahil olduÄŸu iÃ§in tam 10x olmayabilir, ama ciddi fark etmeli
    assert norm1 / norm2 > 2.0, "moe_weight ciddi fark yaratmalÄ±"
    print("âœ… plasticity_moe_weight_scaling")


def test_model_with_moe_passes_weight():
    """Modelin MoE modunda forward_token'Ä±n Ã§Ã¶kmediÄŸini doÄŸrula."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=2, num_deep=2, use_moe=True)
    x = torch.randint(0, 100, (1, 10))
    logits = model(x)
    assert logits.shape == (1, 10, 100)
    print("âœ… model_with_moe_passes_weight")


# â•â•â• 3. Biological RK2 Plausibility â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_rk2_biological_timing():
    """ode_cell iÃ§indeki update_hebb Ã§aÄŸrÄ±sÄ±nÄ±n steps'e uyumunu doÄŸrula."""
    cell1 = LiquidODECell(32, 32, ode_steps=1, use_plasticity=True)
    cell3 = LiquidODECell(32, 32, ode_steps=3, use_plasticity=True)
    
    # Parametreleri eÅŸitle (tau net dahil)
    cell3.load_state_dict(cell1.state_dict())
    
    x = torch.randn(1, 32)
    h = torch.zeros(1, 32)
    
    h1 = cell1(x, h.clone(), adaptive_steps=False)
    # RK2 loop'ta hebb 3 kez gÃ¼ncellenecek, ama step=3 olduÄŸu iÃ§in 1/3 gÃ¼cÃ¼nde
    h3 = cell3(x, h.clone(), adaptive_steps=False)
    
    assert cell1.hebb_info['ih'] > 0
    assert cell3.hebb_info['ih'] > 0
    
    # 3 adÄ±mlÄ± hÃ¼crenin hebb normu 1 adÄ±mlÄ±dan aÅŸÄ±rÄ± bÃ¼yÃ¼k olmamalÄ± (normalize edildiÄŸi iÃ§in)
    ratio = cell3.hebb_info['ih'] / (cell1.hebb_info['ih'] + 1e-8)
    # Ä°dealde ~1 civarÄ± olmalÄ±, RK2 entegrasyon farkÄ±ndan sapma olabilir ama 3x olmamalÄ±
    assert ratio < 2.0, f"RK2 Hebb birikimi anormal: {ratio:.2f}x"
    print("âœ… rk2_biological_timing")


# â•â•â• Combined â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_all_v033_combined():
    """GeliÅŸmiÅŸ ODE, MoE ve Plastisite Ã¶zelliklerinin stabil Ã§alÄ±ÅŸmasÄ±."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=2, num_deep=2,
                           use_moe=True, 
                           tau_gate=True,
                           use_multiscale=True)
    # Training step
    model.train()
    x = torch.randint(0, 100, (1, 10))
    logits = model(x)
    loss = logits.sum() + model._aux_loss
    loss.backward()
    
    assert model.embed.weight.grad is not None
    print("âœ… all_v033_combined")


if __name__ == "__main__":
    test_hebb_capacity_independent_of_w()
    
    test_plasticity_moe_weight_scaling()
    test_model_with_moe_passes_weight()
    
    test_rk2_biological_timing()
    
    test_all_v033_combined()

    print("\nğŸ† TÃ¼m v0.3.3 testleri geÃ§ti!")
