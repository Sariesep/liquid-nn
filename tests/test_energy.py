"""Energy efficiency unit testleri â€” 7 optimizasyon."""

import sys, os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
from liquidnn import (MiniLiquidGPT, RMSNorm, SlidingWindowAttention,
                      quantize_model, model_size_mb)
from liquidnn.plasticity import PlasticSynapse
from liquidnn.ode_cell import LiquidODECell


# â•â•â• 1. RMSNorm â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_rmsnorm_output_shape():
    """RMSNorm doÄŸru ÅŸekil dÃ¶ndÃ¼rmeli."""
    norm = RMSNorm(32)
    x = torch.randn(2, 32)
    out = norm(x)
    assert out.shape == (2, 32)
    print("âœ… rmsnorm_output_shape")


def test_rmsnorm_in_model():
    """use_rmsnorm=True ile model Ã§alÄ±ÅŸmalÄ±."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=1, num_deep=1,
                           use_rmsnorm=True)
    x = torch.randint(0, 100, (1, 10))
    logits = model(x)
    assert logits.shape == (1, 10, 100)
    # LayerNorm yerine RMSNorm kullanÄ±ldÄ±ÄŸÄ±nÄ± doÄŸrula
    assert isinstance(model.norms[0], RMSNorm)
    assert isinstance(model.out_norm, RMSNorm)
    print("âœ… rmsnorm_in_model")


# â•â•â• 2. GQA â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_gqa_reduces_params():
    """GQA (1 KV head) daha az parametreye sahip olmalÄ±."""
    mha = SlidingWindowAttention(embed_dim=32, num_heads=4,
                                  num_kv_heads=None, window_size=8)
    gqa = SlidingWindowAttention(embed_dim=32, num_heads=4,
                                  num_kv_heads=1, window_size=8)

    mha_params = sum(p.numel() for p in mha.parameters())
    gqa_params = sum(p.numel() for p in gqa.parameters())
    assert gqa_params < mha_params, \
        f"GQA ({gqa_params}) MHA'dan ({mha_params}) kÃ¼Ã§Ã¼k olmalÄ±"
    print(f"âœ… gqa_reduces_params: MHA={mha_params}, GQA={gqa_params}")


def test_gqa_in_model():
    """GQA + attention ile model Ã§alÄ±ÅŸmalÄ±."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=1, num_deep=1,
                           use_attention=True, attn_heads=4)
    x = torch.randint(0, 100, (1, 10))
    logits = model(x)
    assert logits.shape == (1, 10, 100)
    print("âœ… gqa_in_model")


# â•â•â• 3. Adaptive ODE Steps â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_adaptive_ode_runs():
    """adaptive_steps=True ile ODE Ã§alÄ±ÅŸmalÄ±."""
    cell = LiquidODECell(32, 32, ode_steps=3, use_plasticity=True)
    x = torch.randn(1, 32)
    h = torch.zeros(1, 32)
    h_out = cell(x, h, enable_plasticity=True, adaptive_steps=True)
    assert h_out.shape == (1, 32)
    print("âœ… adaptive_ode_runs")


def test_adaptive_ode_in_model():
    """adaptive_ode=True ile model Ã§alÄ±ÅŸmalÄ±."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=1, num_deep=1,
                           adaptive_ode=True)
    x = torch.randint(0, 100, (1, 10))
    logits = model(x)
    assert logits.shape == (1, 10, 100)
    print("âœ… adaptive_ode_in_model")


# â•â•â• 4. Sparse Hebb â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_sparse_hebb():
    """sparse_k ile Hebb matrisinde az eleman kalmalÄ±."""
    synapse = PlasticSynapse(32, 32, sparse_k=64)
    x = torch.randn(1, 32)
    _ = synapse(x)  # init Hebb
    synapse.update_hebb(x, torch.randn(1, 32))

    nonzero = (synapse.Hebb != 0).sum().item()
    assert nonzero <= 64, f"sparse_k=64 ama {nonzero} nonzero eleman var"
    print(f"âœ… sparse_hebb: nonzero={nonzero}/1024")


def test_sparse_hebb_default():
    """sparse_k=0 (varsayÄ±lan) â†’ tam yoÄŸun Hebb."""
    synapse = PlasticSynapse(32, 32, sparse_k=0)
    x = torch.randn(1, 32)
    _ = synapse(x)
    synapse.update_hebb(x, torch.randn(1, 32))

    nonzero = (synapse.Hebb != 0).sum().item()
    assert nonzero > 64, f"VarsayÄ±landa Ã§oÄŸu eleman nonzero olmalÄ±, {nonzero}"
    print(f"âœ… sparse_hebb_default: nonzero={nonzero}/1024")


# â•â•â• 5. Early Exit â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_early_exit():
    """early_exit_threshold ile model Ã§alÄ±ÅŸmalÄ±."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=2, num_deep=2,
                           early_exit_threshold=0.5)
    model.eval()
    x = torch.randint(0, 100, (1, 5))

    model.init_hidden(1, x.device)
    logits = model.forward_token(x[:, 0], 0)
    assert logits.shape == (1, 100)
    print("âœ… early_exit")


def test_early_exit_training_disabled():
    """Early exit eÄŸitimde aktif olmamalÄ±."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=2, num_deep=2,
                           early_exit_threshold=0.5)
    model.train()
    x = torch.randint(0, 100, (1, 5))
    logits = model(x)
    assert logits.shape == (1, 5, 100)
    print("âœ… early_exit_training_disabled")


# â•â•â• 6. Gradient Checkpointing â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_gradient_checkpointing():
    """use_checkpointing=True ile forward Ã§alÄ±ÅŸmalÄ±."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=1, num_deep=1)
    model.train()
    x = torch.randint(0, 100, (1, 10))
    # Normal forward (referans)
    logits_normal = model(x, use_checkpointing=False)
    assert logits_normal.shape == (1, 10, 100)

    # Checkpointed forward
    model.init_hidden(1, x.device)
    model.reset_hebb()
    logits_ckpt = model(x, use_checkpointing=True)
    assert logits_ckpt.shape == (1, 10, 100)

    # Backward Ã§alÄ±ÅŸmalÄ± â€” parametrelerde grad olmalÄ±
    loss = logits_ckpt[:, :-1].reshape(-1, 100)
    targets = x[:, 1:].reshape(-1)
    ce = torch.nn.functional.cross_entropy(loss, targets)
    ce.backward()

    has_grad = any(p.grad is not None for p in model.parameters()
                   if p.requires_grad)
    assert has_grad, "Checkpointing sonrasÄ± gradient olmalÄ±"
    print("âœ… gradient_checkpointing")


# â•â•â• 7. INT8 Quantization â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_quantize_model():
    """Quantize sonrasÄ± model daha kÃ¼Ã§Ã¼k olmalÄ±."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=1, num_deep=1)
    size_before = model_size_mb(model)

    model_q = quantize_model(model)
    size_after = model_size_mb(model_q)

    # Quantize edilmiÅŸ model daha kÃ¼Ã§Ã¼k olmalÄ±
    # (kÃ¼Ã§Ã¼k modelde fark az olabilir ama en azÄ±ndan Ã§alÄ±ÅŸmalÄ±)
    assert size_after <= size_before * 1.1, \
        f"Quantize sonrasÄ± boyut artmamalÄ±: {size_before:.2f} â†’ {size_after:.2f}"
    print(f"âœ… quantize_model: {size_before:.2f}MB â†’ {size_after:.2f}MB")


# â•â•â• Combined Test â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_all_features_combined():
    """TÃ¼m optimizasyonlar birlikte Ã§alÄ±ÅŸmalÄ±."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=1, num_deep=1,
                           use_rmsnorm=True,
                           adaptive_ode=True,
                           early_exit_threshold=0.5,
                           use_attention=True, attn_heads=4)
    model.eval()

    prompt = torch.tensor([1, 2, 3])
    out = model.generate(prompt, max_new=5)
    assert out.shape[1] == 8
    print("âœ… all_features_combined")


def test_backward_compat():
    """VarsayÄ±lan parametrelerle mevcut davranÄ±ÅŸ korunmalÄ±."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=1, num_deep=1)
    import torch.nn as nn
    assert isinstance(model.norms[0], nn.LayerNorm), "VarsayÄ±lan LayerNorm"
    assert model.adaptive_ode == False
    assert model.early_exit_threshold == 0.0

    x = torch.randint(0, 100, (2, 10))
    logits = model(x)
    assert logits.shape == (2, 10, 100)
    print("âœ… backward_compat")


if __name__ == "__main__":
    test_rmsnorm_output_shape()
    test_rmsnorm_in_model()

    test_gqa_reduces_params()
    test_gqa_in_model()

    test_adaptive_ode_runs()
    test_adaptive_ode_in_model()

    test_sparse_hebb()
    test_sparse_hebb_default()

    test_early_exit()
    test_early_exit_training_disabled()

    test_gradient_checkpointing()

    test_quantize_model()

    test_all_features_combined()
    test_backward_compat()

    print("\nğŸ† TÃ¼m enerji verimliliÄŸi testleri geÃ§ti!")
