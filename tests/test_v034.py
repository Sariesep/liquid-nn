"""v0.3.4 testleri â€” Biyolojik Plastisite & MoE Ä°yileÅŸtirmeleri."""

import sys, os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
torch.autograd.set_detect_anomaly(True)
import torch.nn.functional as F
from liquidnn import MiniLiquidGPT, Neuromodulator
from liquidnn.ode_cell import LiquidODECell
from liquidnn.plasticity import PlasticSynapse


# â•â•â• 1. NÃ¶romodÃ¼lasyon â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_neuromodulator_signal_range():
    """Mod sinyali makul aralÄ±kta olmalÄ±."""
    nm = Neuromodulator(32, baseline=1.0, sensitivity=1.0)
    
    signals = []
    for _ in range(20):
        err = torch.randn(1, 32) * torch.rand(1).item() * 5
        sig = nm(err)
        signals.append(sig)
    
    assert all(0.1 <= s <= 3.0 for s in signals), \
        f"Sinyal aralÄ±k dÄ±ÅŸÄ±: min={min(signals):.3f}, max={max(signals):.3f}"
    print("âœ… neuromodulator_signal_range")


def test_neuromod_scales_plasticity():
    """YÃ¼ksek mod_signal daha Ã§ok Hebb birikimi Ã¼retmeli."""
    syn_low = PlasticSynapse(16, 16)
    syn_high = PlasticSynapse(16, 16)
    syn_high.load_state_dict(syn_low.state_dict())
    
    pre = torch.randn(1, 16)
    post = torch.randn(1, 16)
    
    syn_low.update_hebb(pre, post, mod_signal=0.5)
    syn_high.update_hebb(pre, post, mod_signal=2.0)
    
    assert syn_high.hebb_norm > syn_low.hebb_norm, \
        "YÃ¼ksek mod_signal daha Ã§ok Hebb gÃ¼ncellemeli"
    print("âœ… neuromod_scales_plasticity")


# â•â•â• 2. Homeostatik Plastisite â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_homeostasis_stabilizes():
    """Homeostasis ile aktivasyonlar hedef deÄŸere yakÄ±nsamalÄ±."""
    target = 0.5
    cell = LiquidODECell(16, 16, ode_steps=1, use_plasticity=False,
                         use_homeostasis=True, target_avg=target)
    h = torch.zeros(2, 16)
    
    # AÅŸÄ±rÄ± bÃ¼yÃ¼k girdilerle bile aktivasyon kontrol altÄ±nda kalmalÄ±
    for _ in range(50):
        x = torch.randn(2, 16) * 5.0  # bÃ¼yÃ¼k girdi
        h = cell(x, h, enable_plasticity=False)
    
    avg_act = h.abs().mean().item()
    # Hedefin 5x'inden bÃ¼yÃ¼k olmamalÄ±
    assert avg_act < target * 5.0, \
        f"Homeostasis baÅŸarÄ±sÄ±z: avg={avg_act:.3f}, hedef={target}"
    print(f"âœ… homeostasis_stabilizes (avg_act={avg_act:.3f})")


# â•â•â• 3. Ã‡ift HÄ±zlÄ± Hebb â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_dual_hebb_independence():
    """Fast ve slow Hebb farklÄ± decay/eta ile farklÄ± normlar Ã¼retmeli."""
    syn = PlasticSynapse(16, 16, use_dual_hebb=True)
    
    pre = torch.randn(2, 16)
    post = torch.randn(2, 16)
    
    for _ in range(10):
        syn.update_hebb(pre, post)
    
    fast_norm = syn.hebb_norm
    slow_norm = syn.hebb_slow_norm
    
    assert fast_norm > 0, "Fast Hebb gÃ¼ncellenmeli"
    assert slow_norm > 0, "Slow Hebb gÃ¼ncellenmeli"
    assert fast_norm != slow_norm, "Fast ve slow farklÄ± olmalÄ±"
    print(f"âœ… dual_hebb_independence (fast={fast_norm:.4f}, slow={slow_norm:.4f})")


def test_dual_hebb_memory_retention():
    """Slow Hebb, fast Hebb'den daha kalÄ±cÄ± olmalÄ±."""
    syn = PlasticSynapse(16, 16, use_dual_hebb=True)
    
    # Ã–ÄŸren
    pre = torch.randn(1, 16)
    post = torch.randn(1, 16)
    for _ in range(20):
        syn.update_hebb(pre, post)
    
    fast_after_learn = syn.hebb_norm
    slow_after_learn = syn.hebb_slow_norm
    
    # SÄ±fÄ±r girdilerle decay uygula (Ã¶ÄŸrenmeden sadece decay)
    zero_pre = torch.zeros(1, 16)
    zero_post = torch.zeros(1, 16)
    for _ in range(50):
        syn.update_hebb(zero_pre, zero_post)
    
    fast_after_decay = syn.hebb_norm
    slow_after_decay = syn.hebb_slow_norm
    
    # Her iki Hebb de azalmÄ±ÅŸ olmalÄ±
    assert fast_after_decay < fast_after_learn, "Fast Hebb decay etmeli"
    assert slow_after_decay < slow_after_learn, "Slow Hebb de decay etmeli"
    
    # Slow daha az kayÄ±p vermeli (gÃ¶receli)
    fast_retention = fast_after_decay / (fast_after_learn + 1e-8)
    slow_retention = slow_after_decay / (slow_after_learn + 1e-8)
    assert slow_retention > fast_retention, \
        f"Slow daha kalÄ±cÄ± olmalÄ±: fast_ret={fast_retention:.3f}, slow_ret={slow_retention:.3f}"
    print(f"âœ… dual_hebb_memory_retention (fast_ret={fast_retention:.3f}, slow_ret={slow_retention:.3f})")


# â•â•â• 4. MoE Expert Capacity Limiting â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_moe_capacity_drops_tokens():
    """Capacity aÅŸÄ±ldÄ±ÄŸÄ±nda bazÄ± tokenler drop edilmeli."""
    from liquidnn.moe import ExpertRouter
    
    router = ExpertRouter(embed_dim=32, num_experts=4, top_k=2,
                          capacity_factor=1.0)
    
    # 8 token â†’ her expert max ceil(8*2/4*1.0) = 4 token alabilir
    x = torch.randn(8, 32)
    weights, indices, aux_loss, dropped_mask = router(x)
    
    assert dropped_mask is not None, "capacity_factor>0 ile dropped_mask olmalÄ±"
    # En azÄ±ndan API doÄŸru Ã§alÄ±ÅŸÄ±yor
    assert dropped_mask.shape == (8, 2)
    print(f"âœ… moe_capacity_drops_tokens (dropped={dropped_mask.sum().item():.0f}/16)")


def test_moe_capacity_in_model():
    """Capacity-limited MoE ile model Ã§alÄ±ÅŸmalÄ±."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=2, num_deep=2,
                           use_moe=True, moe_capacity_factor=1.25)
    x = torch.randint(0, 100, (2, 10))
    logits = model(x)
    assert logits.shape == (2, 10, 100)
    print("âœ… moe_capacity_in_model")


# â•â•â• 5. Sinaptik Konsolidasyon â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_consolidation_protects_important():
    """Konsolidasyon ile Ã¶nemli izler deÄŸiÅŸime daha direnÃ§li olmalÄ±."""
    syn_free = PlasticSynapse(16, 16, use_consolidation=False)
    syn_cons = PlasticSynapse(16, 16, use_consolidation=True,
                               consolidation_strength=5.0)
    syn_cons.load_state_dict(syn_free.state_dict(), strict=False)
    
    # AynÄ± kalÄ±bÄ± 30 kez Ã¶ÄŸret (Ã¶nemli iz oluÅŸtur)
    pre1 = torch.randn(1, 16)
    post1 = torch.randn(1, 16)
    for _ in range(30):
        syn_free.update_hebb(pre1, post1)
        syn_cons.update_hebb(pre1, post1)
    
    snapshot_free = syn_free.Hebb.clone()
    snapshot_cons = syn_cons.Hebb.clone()
    
    # Yeni, farklÄ± kalÄ±p ile Ã¼zerine yaz
    pre2 = torch.randn(1, 16) * 3
    post2 = torch.randn(1, 16) * 3
    for _ in range(30):
        syn_free.update_hebb(pre2, post2)
        syn_cons.update_hebb(pre2, post2)
    
    # Konsolidasyonlu olan orijinalden daha az sapmalÄ±
    drift_free = (syn_free.Hebb - snapshot_free).norm().item()
    drift_cons = (syn_cons.Hebb - snapshot_cons).norm().item()
    
    assert drift_cons < drift_free, \
        f"Konsolidasyon drifi azaltmalÄ±: free={drift_free:.4f}, cons={drift_cons:.4f}"
    print(f"âœ… consolidation_protects_important (drift: free={drift_free:.3f}, cons={drift_cons:.3f})")


# â•â•â• Combined â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_all_v034_combined():
    """TÃ¼m v0.3.4 Ã¶zellikleri aÃ§Ä±k, forward + backward stabil Ã§alÄ±ÅŸmalÄ±."""
    model = MiniLiquidGPT(
        vocab_size=100, embed_dim=32,
        num_fast=1, num_deep=1,
        use_moe=True,
        use_neuromod=True,
        use_homeostasis=True,
        use_dual_hebb=True,
        use_consolidation=True,
        consolidation_strength=1.0,
        moe_capacity_factor=1.5,
    )
    
    # Training step
    model.train()
    x = torch.randint(0, 100, (2, 10))
    logits = model(x)
    loss = logits.sum() + model._aux_loss
    loss.backward()
    
    assert model.embed.weight.grad is not None
    
    # Hebb stats'ta slow anahtarlarÄ± gÃ¶rÃ¼nmeli
    stats = model.hebb_stats()
    has_slow = any('slow' in k for k in stats.keys())
    assert has_slow, "Dual hebb aÃ§Ä±k, slow stats olmalÄ±"
    
    print("âœ… all_v034_combined")


def test_backward_compat_defaults():
    """VarsayÄ±lan parametrelerle v0.3.3 davranÄ±ÅŸÄ± korunmalÄ±."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=1, num_deep=1)
    
    # Yeni Ã¶zellikler varsayÄ±lan kapalÄ±
    assert model.use_neuromod == False
    assert model._neuromod is None
    
    x = torch.randint(0, 100, (2, 10))
    logits = model(x)
    assert logits.shape == (2, 10, 100)
    print("âœ… backward_compat_defaults")


if __name__ == "__main__":
    test_neuromodulator_signal_range()
    test_neuromod_scales_plasticity()
    
    test_homeostasis_stabilizes()
    
    test_dual_hebb_independence()
    test_dual_hebb_memory_retention()
    
    test_moe_capacity_drops_tokens()
    test_moe_capacity_in_model()
    
    test_consolidation_protects_important()
    
    test_all_v034_combined()
    test_backward_compat_defaults()

    print("\nğŸ† TÃ¼m v0.3.4 testleri geÃ§ti!")
