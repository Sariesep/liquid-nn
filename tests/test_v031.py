"""v0.3.1 testleri â€” Flash Attention, KV Cache, RoPE."""

import sys, os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
from liquidnn import MiniLiquidGPT, SlidingWindowAttention
from liquidnn.attention import _precompute_freqs, _apply_rope


# â•â•â• 1. RoPE â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_rope_freqs():
    """RoPE frekans tablosu doÄŸru boyutta olmalÄ±."""
    freqs = _precompute_freqs(head_dim=8, max_len=128)
    assert freqs.shape == (128, 4, 2), f"Beklenen (128,4,2), alÄ±nan {freqs.shape}"
    print("âœ… rope_freqs")


def test_rope_apply():
    """RoPE uygulama sonrasÄ± ÅŸekil korunmalÄ±."""
    freqs = _precompute_freqs(head_dim=8, max_len=128)
    x = torch.randn(1, 4, 1, 8)  # [B, H, 1, Dh]
    out = _apply_rope(x, pos=5, freqs=freqs)
    assert out.shape == x.shape
    # RoPE sonrasÄ± norm yaklaÅŸÄ±k aynÄ± kalmalÄ± (rotasyon)
    assert abs(x.norm().item() - out.norm().item()) < 0.01
    print("âœ… rope_apply")


def test_rope_different_pos():
    """AynÄ± vektÃ¶re farklÄ± pozisyon â†’ farklÄ± Ã§Ä±ktÄ±."""
    freqs = _precompute_freqs(head_dim=8, max_len=128)
    x = torch.randn(1, 4, 1, 8)
    out5 = _apply_rope(x, pos=5, freqs=freqs)
    out50 = _apply_rope(x, pos=50, freqs=freqs)
    assert not torch.allclose(out5, out50), "FarklÄ± pozisyonlar farklÄ± Ã§Ä±ktÄ± vermeli"
    print("âœ… rope_different_pos")


def test_model_with_rope():
    """use_rope=True ile model Ã§alÄ±ÅŸmalÄ±."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=1, num_deep=1,
                           use_attention=True, attn_heads=4,
                           use_rope=True)
    x = torch.randint(0, 100, (1, 10))
    logits = model(x)
    assert logits.shape == (1, 10, 100)
    print("âœ… model_with_rope")


# â•â•â• 2. KV Cache â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_kv_cache_populated():
    """KV cache token ekledikÃ§e dolmalÄ±."""
    attn = SlidingWindowAttention(embed_dim=32, num_heads=4,
                                   window_size=8, use_rope=False)
    for i in range(5):
        x = torch.randn(1, 32)
        attn(x, pos=i)

    assert attn._buf_len == 5
    # K cache de dolu olmalÄ±
    k_nonzero = (attn._k_cache[:, :, :5].abs().sum() > 0).any()
    assert k_nonzero, "KV cache boÅŸ olmamalÄ±"
    print("âœ… kv_cache_populated")


def test_kv_cache_save_restore():
    """KV cache save/restore Ã§alÄ±ÅŸmalÄ±."""
    attn = SlidingWindowAttention(embed_dim=32, num_heads=4,
                                   window_size=8, use_rope=False)
    for i in range(3):
        attn(torch.randn(1, 32), pos=i)

    state = attn.get_buffer_state()

    # Cache'i deÄŸiÅŸtir
    for i in range(5):
        attn(torch.randn(1, 32), pos=3+i)

    # Restore
    attn.set_buffer_state(state)
    assert attn._buf_len == 3
    assert torch.allclose(attn._k_cache, state['k_cache'])
    print("âœ… kv_cache_save_restore")


def test_kv_cache_overflow():
    """Window dolunca en eski eleman atÄ±lmalÄ± (FIFO)."""
    attn = SlidingWindowAttention(embed_dim=32, num_heads=4,
                                   window_size=4, use_rope=False)
    # 6 token besle (4'lÃ¼k window'a)
    for i in range(6):
        attn(torch.randn(1, 32), pos=i)

    assert attn._buf_len == 4, f"Buffer dolu olmalÄ±, {attn._buf_len}"
    print("âœ… kv_cache_overflow")


# â•â•â• 3. Flash Attention â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_flash_attention():
    """use_flash=True ile attention Ã§alÄ±ÅŸmalÄ±."""
    attn = SlidingWindowAttention(embed_dim=32, num_heads=4,
                                   window_size=8, use_flash=True)
    for i in range(5):
        out = attn(torch.randn(1, 32), pos=i)
    assert out.shape == (1, 32)
    print("âœ… flash_attention")


def test_flash_vs_manual_close():
    """Flash ve manual attention yakÄ±n sonuÃ§ vermeli."""
    torch.manual_seed(42)
    flash_attn = SlidingWindowAttention(embed_dim=32, num_heads=4,
                                         window_size=8,
                                         use_flash=True, use_rope=False)
    torch.manual_seed(42)
    manual_attn = SlidingWindowAttention(embed_dim=32, num_heads=4,
                                          window_size=8,
                                          use_flash=False, use_rope=False)

    # AÄŸÄ±rlÄ±klarÄ± eÅŸitle
    manual_attn.load_state_dict(flash_attn.state_dict())
    flash_attn.eval()
    manual_attn.eval()

    tokens = [torch.randn(1, 32) for _ in range(5)]

    for i, tok in enumerate(tokens):
        out_flash = flash_attn(tok, pos=i)
        out_manual = manual_attn(tok, pos=i)

    # Son Ã§Ä±ktÄ±lar yakÄ±n olmalÄ±
    assert torch.allclose(out_flash, out_manual, atol=1e-4), \
        f"Flash/manual fark: {(out_flash - out_manual).abs().max():.6f}"
    print("âœ… flash_vs_manual_close")


# â•â•â• Combined â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_all_v031_features():
    """RoPE + Flash + KV Cache ile speculative decoding Ã§alÄ±ÅŸmalÄ±."""
    main = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                          num_fast=1, num_deep=1,
                          use_attention=True, attn_heads=4,
                          use_rope=True, use_flash=True)
    draft = MiniLiquidGPT.create_draft_model(main)

    prompt = torch.tensor([1, 2, 3])
    out = main.generate_speculative(draft, prompt, max_new=10, gamma=3)
    assert out.shape[1] == 13
    print("âœ… all_v031_features")


def test_backward_compat_attention():
    """VarsayÄ±lan parametrelerle eski attention davranÄ±ÅŸÄ± korunmalÄ±."""
    attn = SlidingWindowAttention(embed_dim=32, num_heads=4,
                                   window_size=8)
    # VarsayÄ±lan: use_rope=True, use_flash=True
    assert attn.use_rope == True
    assert attn.use_flash == True

    x = torch.randn(1, 32)
    out = attn(x, pos=0)
    assert out.shape == (1, 32)
    print("âœ… backward_compat_attention")


def test_generate_with_rope():
    """Normal generate RoPE ile Ã§alÄ±ÅŸmalÄ±."""
    model = MiniLiquidGPT(vocab_size=100, embed_dim=32,
                           num_fast=1, num_deep=1,
                           use_attention=True, use_rope=True)
    prompt = torch.tensor([1, 2, 3])
    out = model.generate(prompt, max_new=8)
    assert out.shape[1] == 11
    print("âœ… generate_with_rope")


if __name__ == "__main__":
    # RoPE
    test_rope_freqs()
    test_rope_apply()
    test_rope_different_pos()
    test_model_with_rope()

    # KV Cache
    test_kv_cache_populated()
    test_kv_cache_save_restore()
    test_kv_cache_overflow()

    # Flash Attention
    test_flash_attention()
    test_flash_vs_manual_close()

    # Combined
    test_all_v031_features()
    test_backward_compat_attention()
    test_generate_with_rope()

    print("\nğŸ† TÃ¼m v0.3.1 testleri geÃ§ti!")
