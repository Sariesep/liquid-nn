# ğŸ§  Liquid Neural Networks â€” Real-Time Learning with Plastic Synapses

> **GPT/Gemini are static. Training ends, they freeze. This model is alive â€” it changes its synapses on every token.**

[![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://www.python.org/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0%2B-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)

---

## ğŸ”¬ What Is This?

A language model research project built from scratch, based on Liquid Time-Constant Networks + Differentiable Hebbian Plasticity.

### How We Differ from Transformers

| | Transformer (GPT/Gemini) | Liquid Neural Network (Ours) |
|---|---|---|
| **Synapses** | Fixed (freeze after training) | Plastic (updated on every token) |
| **Memory** | Context window (temporary) | Hebbian traces (persistent) |
| **Adaptation** | Requires fine-tuning (hours) | Real-time (milliseconds) |
| **Computation** | Fixed depth | Adaptive ODE steps (easyâ†’fast, hardâ†’deep) |

### Architecture

```
Token â†’ Embed(50257, 256) + SinPosEnc
  â†’ LiquidODE Ã— 2 (steps=1, Euler â€” fast perception)
  â†’ LiquidODE Ã— 2 (steps=3, RK2 + Hebb â€” deep reasoning)
  â†’ Head (weight-tied) â†’ Logits
```

## ğŸš€ Quick Start

```bash
# Clone
git clone https://github.com/YOUR_USERNAME/liquid-nn.git
cd liquid-nn

# Install dependencies
pip install -r requirements.txt

# Train (Colab T4 or local GPU)
python scripts/train.py --config configs/base.yaml

# Generate text
python scripts/generate.py --checkpoint checkpoints/best_model.pt --prompt "The meaning of life"

# Plasticity test
python scripts/plasticity_test.py --checkpoint checkpoints/best_model.pt
```

### Run on Google Colab

```python
!git clone https://github.com/YOUR_USERNAME/liquid-nn.git
%cd liquid-nn
!pip install -r requirements.txt
!python scripts/train.py --config configs/colab_t4.yaml
```

## ğŸ“ Project Structure

```
liquid-nn/
â”œâ”€â”€ liquidnn/                # Main library (pip installable)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ plasticity.py        # PlasticSynapse â€” Hebbian learning
â”‚   â”œâ”€â”€ ode_cell.py          # LiquidODECell â€” Liquid neuron
â”‚   â”œâ”€â”€ model.py             # MiniLiquidGPT â€” Main model
â”‚   â”œâ”€â”€ tokenizer.py         # tiktoken wrapper
â”‚   â””â”€â”€ utils.py             # Utility functions
â”œâ”€â”€ configs/                 # Training configurations
â”‚   â”œâ”€â”€ base.yaml            # Default settings
â”‚   â”œâ”€â”€ colab_t4.yaml        # Colab T4 optimized
â”‚   â”œâ”€â”€ small.yaml           # Quick experiments (~5M params)
â”‚   â””â”€â”€ large.yaml           # Large model (~50M params)
â”œâ”€â”€ scripts/                 # Executable scripts
â”‚   â”œâ”€â”€ train.py             # Training
â”‚   â”œâ”€â”€ generate.py          # Text generation
â”‚   â”œâ”€â”€ plasticity_test.py   # ZEPHYR / Bloop test
â”‚   â””â”€â”€ benchmark.py         # Performance measurement
â”œâ”€â”€ data/                    # Data loading
â”‚   â””â”€â”€ loader.py
â”œâ”€â”€ tests/                   # Unit tests
â”‚   â”œâ”€â”€ test_plasticity.py
â”‚   â”œâ”€â”€ test_ode_cell.py
â”‚   â””â”€â”€ test_model.py
â”œâ”€â”€ notebooks/               # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_quickstart.ipynb
â”‚   â”œâ”€â”€ 02_plasticity_demo.ipynb
â”‚   â””â”€â”€ 03_training.ipynb
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ architecture.md
â”‚   â””â”€â”€ plasticity.md
â”œâ”€â”€ checkpoints/             # Model weights (not in git)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## ğŸ“Š Results

| Metric | Value |
|---|---|
| Parameters | ~14M |
| Val Perplexity | ... |
| Plasticity ON vs OFF | ... |
| ZEPHYR Persistence | ... |

*Results will be updated as training completes.*

## ğŸ”¬ Research Notes

This project is inspired by the following papers:
- [Liquid Time-constant Networks](https://arxiv.org/abs/2006.04439) (Hasani et al., 2020)
- [Differentiable Plasticity](https://arxiv.org/abs/1804.02464) (Miconi et al., 2018)
- [Neural ODEs](https://arxiv.org/abs/1806.07366) (Chen et al., 2018)

## ğŸ“ License

MIT License â€” Use, modify, and share as you like.

## ğŸ¤ Contributing

Pull requests are welcome! Help is especially needed on:
- [ ] Larger datasets (TinyStories, Cosmopedia)
- [ ] Multi-head plasticity
- [ ] Benchmark comparisons (GPT-2 small vs Liquid)
- [ ] ONNX/TensorRT export
- [ ] Mobile deployment (CoreML, NNAPI)
