"""
LiquidNN v0.3.4 â€” Google Colab HÄ±zlÄ± EÄŸitim Demo
==================================================
Bu script'i Colab'a kopyalayÄ±p Ã§alÄ±ÅŸtÄ±rÄ±n.
GPU runtime seÃ§meyi unutmayÄ±n: Runtime â†’ Change runtime type â†’ T4 GPU

Tahmini sÃ¼re: ~3-5 dakika (T4 GPU ile)
"""

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 1: Kurulum                                            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# !pip install tiktoken -q
# !git clone https://github.com/Sariesep/liquid-nn.git
# %cd liquid-nn

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 2: Import & Cihaz                                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import torch
import torch.nn.functional as F
import time
import math

# EÄŸer Colab'da "liquid-nn" dizinine cd yaptÄ±ysanÄ±z:
import sys, os
sys.path.insert(0, os.path.abspath('.'))

from liquidnn import MiniLiquidGPT, get_tokenizer

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  Cihaz: {device}")
if device.type == 'cuda':
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 3: EÄŸitim Verisi (KÃ¼Ã§Ã¼k TÃ¼rkÃ§e Corpus)               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# KÄ±sa demo metni â€” gerÃ§ek eÄŸitimde daha bÃ¼yÃ¼k corpus kullanÄ±lmalÄ±
CORPUS = """
Yapay zeka, makinelerin insan benzeri zeka sergilemesini saÄŸlayan bir bilim dalÄ±dÄ±r.
Derin Ã¶ÄŸrenme, yapay sinir aÄŸlarÄ±nÄ±n Ã§ok katmanlÄ± yapÄ±larla karmaÅŸÄ±k kalÄ±plarÄ± Ã¶ÄŸrenmesini saÄŸlar.
NÃ¶ral aÄŸlar, beyindeki nÃ¶ronlarÄ±n Ã§alÄ±ÅŸma prensibinden esinlenerek geliÅŸtirilmiÅŸtir.
SÄ±vÄ± nÃ¶ral aÄŸlar, zaman deÄŸiÅŸkenli diferansiyel denklemlerle sÃ¼rekli adaptasyon saÄŸlar.
Hebbian Ã¶ÄŸrenme kuralÄ±, birlikte ateÅŸleyen nÃ¶ronlarÄ±n baÄŸlantÄ±larÄ±nÄ±n gÃ¼Ã§lendiÄŸini sÃ¶yler.
Transformer mimarisi, dikkat mekanizmasÄ± ile uzun menzilli baÄŸÄ±mlÄ±lÄ±klarÄ± yakalar.
Ã–ÄŸrenme hÄ±zÄ±, modelin aÄŸÄ±rlÄ±klarÄ±nÄ± ne kadar hÄ±zlÄ± gÃ¼ncellediÄŸini belirler.
Gradient iniÅŸ yÃ¶ntemi, kayÄ±p fonksiyonunu minimize etmek iÃ§in kullanÄ±lÄ±r.
AÅŸÄ±rÄ± Ã¶ÄŸrenme, modelin eÄŸitim verisini ezberlemesi ve genelleme yapamamasÄ± durumudur.
DÃ¼zenlileÅŸtirme teknikleri, modelin genelleme kapasitesini artÄ±rmak iÃ§in uygulanÄ±r.
Dikkat mekanizmasÄ±, girdi sekansÄ±nÄ±n farklÄ± bÃ¶lÃ¼mlerine farklÄ± aÄŸÄ±rlÄ±klar verir.
Geri yayÄ±lÄ±m algoritmasÄ±, hata sinyalini aÄŸ boyunca geriye doÄŸru yayarak Ã¶ÄŸrenmeyi saÄŸlar.
Batch normalizasyon, eÄŸitim sÃ¼recini hÄ±zlandÄ±ran ve kararlÄ± hale getiren bir tekniktir.
KonvolÃ¼syonel aÄŸlar, gÃ¶rÃ¼ntÃ¼ tanÄ±ma ve bilgisayarlÄ± gÃ¶rÃ¼ alanÄ±nda devrim yaratmÄ±ÅŸtÄ±r.
DoÄŸal dil iÅŸleme, bilgisayarlarÄ±n insan dilini anlamasÄ±nÄ± ve Ã¼retmesini hedefler.
PekiÅŸtirmeli Ã¶ÄŸrenme, bir ajanÄ±n deneme yanÄ±lma yoluyla en iyi stratejiyi Ã¶ÄŸrenmesidir.
Transfer Ã¶ÄŸrenme, bir gÃ¶revde Ã¶ÄŸrenilen bilginin baÅŸka bir gÃ¶reve aktarÄ±lmasÄ±dÄ±r.
Ãœretici Ã§ekiÅŸmeli aÄŸlar, gerÃ§ekÃ§i veri Ã¼retmek iÃ§in iki aÄŸÄ±n rekabet etmesini kullanÄ±r.
Otomatik kodlayÄ±cÄ±lar, veriyi sÄ±kÄ±ÅŸtÄ±rÄ±p yeniden oluÅŸturarak Ã¶zellik Ã§Ä±karmayÄ± Ã¶ÄŸrenir.
Metin Ã¼retimi, dil modellerinin olasÄ±lÄ±ksal daÄŸÄ±lÄ±mlardan yeni metinler oluÅŸturmasÄ±dÄ±r.
""".strip()

# Tokenize
tokenizer = get_tokenizer()
tokens = tokenizer.encode(CORPUS)
data = torch.tensor(tokens, dtype=torch.long, device=device)
print(f"ğŸ“ Corpus: {len(CORPUS)} karakter â†’ {len(tokens)} token")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 4: Model OluÅŸtur (v0.3.4 tÃ¼m Ã¶zellikler aÃ§Ä±k)        â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# NOT: use_attention=False â€” Attention modÃ¼lÃ¼nÃ¼n KV cache'i
# eÄŸitim sÄ±rasÄ±nda autograd in-place hatasÄ± verebilir.
# Inference'ta (generate) attention gÃ¼venle kullanÄ±labilir.
# v0.3.4'Ã¼n asÄ±l yenilikleri (neuromod, homeostasis, dual hebb,
# consolidation) ODE + plastisite katmanlarÄ±ndadÄ±r.

model = MiniLiquidGPT(
    vocab_size=tokenizer.vocab_size,
    embed_dim=128,
    num_fast=2,
    num_deep=2,
    fast_steps=1,
    deep_steps=3,
    dropout=0.1,
    max_seq=512,
    # â”€â”€ Attention (eÄŸitimde KAPALI â€” KV cache autograd sorunu) â”€â”€
    use_attention=False,
    # â”€â”€ MoE â”€â”€
    use_moe=False,          # kÃ¼Ã§Ã¼k modelde MoE gereksiz
    # â”€â”€ v0.3.4 â”€â”€
    use_neuromod=True,       # âœ… NÃ¶romodÃ¼lasyon
    use_homeostasis=True,    # âœ… Homeostatik Plastisite
    homeostasis_target=0.5,
    use_dual_hebb=True,      # âœ… Ã‡ift HÄ±zlÄ± Hebb
    use_consolidation=True,  # âœ… Sinaptik Konsolidasyon
    consolidation_strength=1.0,
    # â”€â”€ DiÄŸer â”€â”€
    use_rmsnorm=True,
    tau_gate=True,
).to(device)

total_params = sum(p.numel() for p in model.parameters())
print(f"\nğŸ§  Model: MiniLiquidGPT v0.3.4")
print(f"   Parametreler: {total_params:,} ({total_params/1e6:.2f}M)")
print(f"   Katmanlar: {model.num_layers} (2 fast + 2 deep)")
print(f"   Ã–zellikler: Neuromod âœ…  Homeostasis âœ…  DualHebb âœ…  Consolidation âœ…")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 5: EÄŸitim DÃ¶ngÃ¼sÃ¼                                    â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Hiperparametreler
EPOCHS = 150
SEQ_LEN = 64
BATCH_SIZE = 4
LR = 3e-4
CHUNK_SIZE = 16

optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)

def make_batch(data, seq_len, batch_size):
    """Rastgele batch Ã¼ret."""
    max_start = len(data) - seq_len - 1
    if max_start <= 0:
        starts = [0] * batch_size
    else:
        starts = torch.randint(0, max_start, (batch_size,))
    x = torch.stack([data[s:s+seq_len] for s in starts])
    y = torch.stack([data[s+1:s+seq_len+1] for s in starts])
    return x, y

print("\n" + "â•" * 60)
print("  EÄÄ°TÄ°M BAÅLIYOR")
print("â•" * 60)

model.train()
start_time = time.time()
losses = []

for epoch in range(1, EPOCHS + 1):
    x_batch, y_batch = make_batch(data, SEQ_LEN, BATCH_SIZE)

    logits = model(x_batch, enable_plasticity=True, chunk_size=CHUNK_SIZE)
    loss = F.cross_entropy(
        logits.reshape(-1, logits.size(-1)),
        y_batch.reshape(-1)
    )

    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    optimizer.step()
    scheduler.step()

    losses.append(loss.item())

    if epoch % 10 == 0 or epoch == 1:
        elapsed = time.time() - start_time
        avg_loss = sum(losses[-10:]) / len(losses[-10:])
        ppl = math.exp(min(avg_loss, 20))  # overflow korumasÄ±
        lr_now = scheduler.get_last_lr()[0]

        # Hebb istatistikleri
        stats = model.hebb_stats()
        hebb_summary = ""
        for k, v in stats.items():
            if v > 0:
                hebb_summary += f"{k}={v:.3f} "

        print(f"  Epoch {epoch:4d}/{EPOCHS} â”‚ "
              f"Loss: {avg_loss:.4f} â”‚ PPL: {ppl:8.1f} â”‚ "
              f"LR: {lr_now:.2e} â”‚ "
              f"â± {elapsed:.0f}s")
        if hebb_summary:
            print(f"           â”‚ Hebb: {hebb_summary.strip()}")

total_time = time.time() - start_time
print("â•" * 60)
print(f"  EÄÄ°TÄ°M TAMAMLANDI â€” {total_time:.1f} saniye")
print(f"  Son Loss: {losses[-1]:.4f} â”‚ PPL: {math.exp(min(losses[-1], 20)):.1f}")
print("â•" * 60)

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 6: Metin Ãœretimi                                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROMPTS = [
    "Yapay zeka",
    "NÃ¶ral aÄŸlar",
    "Derin Ã¶ÄŸrenme",
    "SÄ±vÄ± nÃ¶ral",
]

print("\n" + "â•" * 60)
print("  METÄ°N ÃœRETÄ°MÄ°")
print("â•" * 60)

model.eval()
for prompt_text in PROMPTS:
    prompt_ids = torch.tensor(
        tokenizer.encode(prompt_text), dtype=torch.long, device=device
    )
    with torch.no_grad():
        out_ids = model.generate(
            prompt_ids, max_new=40,
            temperature=0.8, top_k=30,
            enable_plasticity=True
        )
    generated = tokenizer.decode(out_ids[0].tolist())
    print(f"\n  ğŸ’¬ \"{prompt_text}\" â†’")
    print(f"     {generated}")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  CELL 7: Model Diagnostik (v0.3.4 Ã–zellikleri)              â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "â•" * 60)
print("  v0.3.4 DÄ°AGNOSTÄ°K")
print("â•" * 60)

# Hebb istatistikleri
stats = model.hebb_stats()
print("\n  ğŸ“Š Hebb NormlarÄ±:")
for key, val in stats.items():
    bar = "â–ˆ" * int(min(val * 50, 40))
    print(f"     {key:15s}: {val:.4f}  {bar}")

# Parametre sayÄ±mÄ±
pcnt = model.count_params()
print(f"\n  ğŸ“ Parametre DaÄŸÄ±lÄ±mÄ±:")
print(f"     Toplam:    {pcnt['total']:>10,}")
print(f"     Embedding: {pcnt['embed']:>10,}")
print(f"     ODE Cells: {pcnt['cells']:>10,}")
print(f"     DiÄŸer:     {pcnt['other']:>10,}")

print("\nâœ… Demo tamamlandÄ±!")
